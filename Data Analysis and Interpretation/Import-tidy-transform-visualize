#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Updated : on Thu Oct 19 22:56:26 2017
@author: annick-eudes
""" 
"""
# --------------------------------------------------------------------------------------------
#                 PART 1. DATA ANALYSIS AND VISUALISATION
# --------------------------------------------------------------------------------------------

# ------------------------------- PRELIMINARIES  --------------------------------

# First thing set the working directory - it is done by setting it in the folder
# icon to the right;

# Next step is to import all the library we will need
#%%
# Libraries

import pandas as pd
import numpy as np
import seaborn as sns                 # for plots
import matplotlib.pyplot as plt # as plt is sort of a nickname for the library because
                                # it is too long.
import statsmodels.formula.api as smf # statsmodels
import statsmodels.stats.multicomp as multi # statsmodels and posthoc test
import scipy.stats              # For the Chi-Square test of independance

#%%
# IMPORTANT BUG FIXS !!!

# Because, by default, the Pandas library often displays an abbreviated list of rows and columns
# from our data frame. And I know the number of values for numsigmo_est is fairly long, 
# We are going to add additional set option statements following the library import syntax 
# that requests a display of the maximum number of rows and columns. 

# The default in Python, limits this display to a subset of the data frame, 
# and so including display, max, columns, or rows, none, removes that limit 
# and allows all rows and columns to be displayed.

#Set PANDAS to show all columns in DataFrame
pd.set_option('display.max_columns', None)
#Set PANDAS to show all rows in DataFrame
pd.set_option('display.max_rows', None)

# bug fix for display formats to avoid run time errors
pd.set_option('display.float_format', lambda x:'%f'%x)

#%%
# Importing the data set:

df = pd.read_csv("ool_pds.csv", low_memory = False)

# Because Python is treating the variables has string instead of numeric variables
# we will convert them as numeric with the following function



#%%
""" setting variables you will be working with to numeric
10/29/15 note that the code is different from what you see in the videos
 A new version of pandas was released that is phasing out the convert_objects(convert_numeric=True)
It still works for now, but it is recommended that the pandas.to_numeric function be
used instead """

""" These where the old codes :
df["W1_G2"] = df["W1_G2"].convert_objects(convert_numeric = True)
df["W1_P20"] = df["W1_P20"].convert_objects(convert_numeric = True)
df["W1_F1"] = df["W1_F1"].convert_objects(convert_numeric = True)"""

# New codes
df["W1_G2"] = pd.to_numeric(df["W1_G2"], errors = "coerce")
df["W1_P20"] = pd.to_numeric(df["W1_P20"], errors = "coerce")
df["W1_F1"] = pd.to_numeric(df["W1_F1"], errors = "coerce")

# The research question is :
# To what extent is the perception of the US situation (W1_G2) associated with the level of income (W1_P20)?

# The variables of interest in our research question
print("W1_P20 is the Personnal Annual income")
print("W1_G2 is the US economy's situation")
print("W1_F1 is the Percentage of how the respondants think about the future")

# Determining the number of rows and columns in the dataset
print("This is the number of observations in the dataset:")
print(len(df))            # Number of observations = rows

print("This is the number of variables in the dataset:")
print(len(df.columns))    # Number of variables = columns

#%%

# -------------------------- Section # 1 / Basis descriptive data analysis ----------------------------


# ---------------------------- Examination of frequency tables  ------------------------------------

# Explatory data analysis ~ starting with one variable
# Univariate analysis
# The 'dropna = False' argument will display the missing values
# Making simple frequency tables [counts and frequencies].

# Counts :

print("Counts of the us economic situation 1 = Better, 2 = About the same, 3 = Worse, -1 = Refused to answer:")
c1 = df["W1_G2"].value_counts(sort=False, dropna = False)
print (c1)

print("Count of personal annual income :")
c2 = df["W1_P20"].value_counts(sort=False, dropna = False)
print (c2)

print("Counts of when you think about your future, are you generally optimistic, pessimistic, or neither optimistic nor pessimistic?")
c3 = df["W1_F1"].value_counts(sort=False, dropna = False)
print(c3)

# frequencies

print("Percentage of the us economic situation : 1 = Better, 2 = About the same, 3 = Worse, -1 = Refused to answer")
p1 = df["W1_G2"].value_counts(sort=False, normalize = True)
print (p1)

print("Percentage of personal annual income :")
p2 = df["W1_P20"].value_counts(sort=False, normalize = True)
print (p2)

print("Percentage of how the respondants think about the future")
p3 = df["W1_F1"].value_counts(sort=False, normalize = True)
print(p3)


# There are otherways to do this to have the same results, by using the .groupby function
print("Counts of the us economic situation 1 = Better, 2 = About the same, 3 = Worse, -1 = Refused to answer:")
ct1 = df.groupby("W1_G2").size()
print(ct1)

print("Count of personal annual income :")
ct2 = df.groupby("W1_P20").size()
print(ct2)

print("Counts of when you think about your future, are you generally optimistic, pessimistic, or neither optimistic nor pessimistic?")
ct3 = df.groupby("W1_F1").size()
print(ct3)

# To have the frequency, the code is simmilar, we just need to had the *100/len(Data)
print("Percentage of the us economic situation : 1 = Better, 2 = About the same, 3 = Worse, -1 = Refused to answer")
ct4 = df.groupby("W1_G2").size()*100/len(df)
print(ct4)

print("Percentage of personal annual income :")
ct5 = df.groupby("W1_P20").size()*100/len(df)
print(ct5)

print("Percentage of when you think about your future, are you generally optimistic, pessimistic, or neither optimistic nor pessimistic?")
ct6 = df.groupby("W1_F1").size()*100/len(df)
print(ct6)

#%%
# -------------- Section # 2 / regarding our research ~ Making data management decisions --------------

# Data Management

# Data management ~ Making decions about the data
# 1rst decide to code or not the missing values
# 2ed Creating or not new variables

# We will not be working with a subset of the data, because our research question, and
# our hypothesis, needs to have a view of the optimisum in regards to the income level.
# 1rst we will code the missing values
# we are going to set responses of (-1 ~ Refused)  for these variables to missing,
# so that Python disregards these values. We will code the missing values (nan).


# ------------------ Coding or recoding missing values ----------------------------------------

print("Let's start the Data Management ~ decision about the data, missing values and creating secondary variables")
df["W1_P20"]=df["W1_P20"].replace(-1, np.nan)
df["W1_G2"]=df["W1_G2"].replace(-1, np.nan)
df["W1_F1"]=df["W1_F1"].replace(-1, np.nan)

# Let's have a look at the variables with the new managed variables compared to the original variables
# The 'dropna = False' argument will display the missing values

print("Count of personal annual income (with the recoding of the missing values): ")
c2 = df["W1_P20"].value_counts(sort=False, dropna = False)
print(c2)

print("Counts of When you think about your future, are you generally 1 = optimistic, 2 = neither, or 3 = pessimistic? (with the recoding of the missing values)")
opt = df["W1_F1"].value_counts(sort=False, dropna = False)
print(opt)

print("Percentage of the us economic situation : 1 = Better, 2 = About the same, 3 = Worse, -1 = Refused to answer (with the recoding of the missing values)")
p1 = df["W1_G2"].value_counts(sort=False, dropna = True)
print(p1)


#%%
# We chose to group values within individual variables for the W1_P20 variable representing
# income level.
# categorize quantitative variable based on customized splits are done by using cut function
# we split the variable into 4 groups (1-7, 8-11, 12-15, 16-19)
# remember that Python starts counting from 0, not 1

# --------------------------- Grouping values within individual variables --------------------------

print("The income level is divided into 4 groups : 1-7 (5k- 24k), 8-11(25k-49k) 12-15(50k-99k), 16-19 (100k-175k or more))")
df["W1_P20"] = pd.cut(df.W1_P20, [0, 7, 11, 15, 19])
c10 = df["W1_P20"].value_counts(sort = False, dropna = True)
print(c10)

#%%

# --------------------------- Counts of the variables ------------------------------
# For verification purposes

print("Counts of the us economic situation 1 = Better, 2 = About the same, 3 = Worse, -1 = Refused to answer:")
c1 = df["W1_G2"].value_counts(sort=False)
print (c1)

print("Count of personal annual income :")
c2 = df["W1_P20"].value_counts(sort=False)
print (c2)

print("Counts of When you think about your future, are you generally 1 = optimistic, 2 = neither, or 3 = pessimistic?")
opt = df["W1_F1"].value_counts(sort=False)
print(opt)

# These variables have been managed

#%%

#  ---------------------------- Section # 3 / Visualising data ~ Graphs -------------------------------

# Visualizing categorical variables - IMPORTANT
# in order for categorical variables to be ordered properly on the horizontal, or X axis, of
# the univariate graph, we should convert your categorical variables, which are often formatted
# as numeric variables, into a format that Python recognizes as categorical.

# In our research question, we have W1_F1 (view on economic situation) has a categorical variable
# and W1_F1, view of the respondants about the future, and W1_P20 witch is a ordinal variable (Still)
# a type of categorical variable

# We will convert the variables to categoric to provide the descriptive statistics
df["W1_G2"] = df["W1_G2"].astype('category')
df["W1_F1"] = df["W1_F1"].astype('category')
df["W1_P20"] = df["W1_P20"].astype("category")


# Standard deviation and other descriptive statistics are for quantitative variables
print("Categorical variable's descriptive statistics are counts, top frequencies ...:")
print("Describe the views of the economy's outcome")
desc1 = df["W1_G2"].describe()
print(desc1)

print("Describe the views on the future by respondants")
desc2 = df["W1_F1"].describe()
print(desc2)

print("Describe the personnal annual income for the respondants")
desc3 = df["W1_P20"].describe()
print(desc3)

#%%

# UNIVARIATE GRAPH FOR CATEGORICAL VARIABLES

# Let's plot our categorical variables :
sns.countplot(x = "W1_G2", data = df)
plt.xlabel("-1 = refused, 1 = better, 2 = about the same, or 3 = worse")
plt.title("Respondants views on the nation's economy compared to one year ago")

#%%

# UNIVARIATE GRAPH
# Graphique pour une variable : "Respondants views regardgin their future :

sns.countplot(x = "W1_F1", data = df)
plt.xlabel("-1 = refused, 1 = optimistic, 2 = neither optimistic nor pessimistic, 3 = pessimistic")
plt.title("Respondants views regarding their future")

#%%
# UNIVARIATE GRAPH
# Graphique pour une variable : Income groups reported by respondents :

sns.countplot(x = "W1_P20", data = df)
plt.xlabel("Interval of annual income :1-7 (5k- 24k), 8-11(25k-49k) 12-15(50k-99k), 16-19 (100k-175k or more)")
plt.title("Income groups reported by respondents")

#%%
# Univariate quantitative variable
# For quantitative variable, the syntax we use in the Python program is slightly different.
# We'll use this quantitative variable :

print("W1_D1 is the variable is How would you rate the president at the time (Barack Obama)")

sns.distplot(df["W1_D1"].dropna(), kde=False);
plt.xlabel("Rating of the US persident")
plt.title("This is a plot rating the US President")
print("This will give a histogram")


#%%
# Now let's display the graphics for the managed variables
# Graphing a quatitative variable
# The W1_P20 is not a ordinal variable, this is of example only
# seaborn.distplot(Data["W1_P20"].dropna(), kde = False)
# plt.xlabel("Group of personal annual income")
# plt.title("Income groups reported by respondents")

# Standard deviation and other descriptive statistics for quantitative variables
print("Describe the views of the economy's outcome")
desc1 = df["W1_G2"].describe()
print(desc1)

print("Describe the views on the future by respondants")
desc2 = df["W1_F1"].describe()
print(desc2)

print("Describe the personnal annual income for the respondants")
desc3 = df["W1_P20"].describe()
print(desc3)

#%%
# we're going to be visualizing our association of interest by exploring the relationship between two variables. 

# ------------------ Make a decision about the role that each variable will play -----
#
# The explanatory variable is the  income level (X = W1_P20) the perception of the and the response
# variable nation’s economic situation (Y = W1_G2 and/or W1_F1). Thus, using the graphing decisions
# flow chart we will use a Categorical to Categorical bar chart to plot the associations between
# our explanatory and response variables.
# We have to convert the categorical variables to numeric to do a C -> C bar chart.

# A effacer
# When doing the visualisations, the variables have to be in numeric (* sinon cela ne va pas marcher)
# Pour éviter ce problème, il est mieux de reconvertir les variables qui étaient en catégorie
# En des variables numériques.

# NOTE: for a bivariate graph, where our response variable (Y) is categorical, we will actually 
# need to convert this categorical response variable back to numeric. 
# This is because the bivariate
# graph displays a mean on the y axis.

# Setting variables you will be working with to numeric

# Ancienne facon de faire :
#df["W1_P20"] = df["W1_P20"].convert_objects(convert_numeric=True)
#df["W1_G2"] = df["W1_G2"].convert_objects(convert_numeric=True)

# Pandas a été mis à jour, maintenant, il faut utiliser :

#df["W1_F1"] = pd.to_numeric(df["W1_F1"], errors="coerce")
#df["W1_P20"] = pd.to_numeric(df["W1_P20"], errors="coerce")
#df["W1_G2"] = pd.to_numeric(df["W1_G2"], errors="coerce")

#%%


# BI VARIATE GRAPH :

# Personnal income (Y = W1_G2) versus the US economy's situation ()
# Convert our Y response variable back to numeric
df["W1_G2"] = pd.to_numeric(df["W1_G2"], errors="coerce")

print("This is the Categorical -> Categorical graph of US economy's situtation vs Personnal annual income")
sns.factorplot(x = "W1_P20", y = "W1_G2", data = df, kind = "bar", ci = None) # ci=None suppresses error bars.
plt.xlabel("Personnal annual income")
plt.ylabel("The US economy's situation")


#%%
# BI VARIATE GRAPH :

# Convert our Y response variable back to numeric
df["W1_F1"] = pd.to_numeric(df["W1_F1"], errors="coerce")
# Personnal income versus how respondants think about the future

sns.factorplot(x = "W1_P20", y = "W1_F1", data = df, kind = "bar", ci = None)
plt.xlabel("Personnal annual income")
plt.ylabel("How the respondants think about the future")

#%%
# Graphing quantitative variables
# We will use a scatter plot

print("W1_D17: [Black Nationalists] How would you rate:")
print("W1_D1 is the variable is How would you rate the president at the time (Barack Obama)")

scater_plot1 = sns.regplot(x = "W1_D17", y = "W1_D1", fit_reg = False, data = df)
plt.xlabel("How would you rate the president")
plt.ylabel("Fatings from Black NAtionalists")
plt.title("Scatter plot for the association between the rating of the US President by Black Nationalists")
# We can add a fitting plot by removing the fit_reg = False

#%%
# bivariate graph, that is the categorical to quantitative bar chart. 





#%%
# --------------------------------------------------------------------------------------------
#                               PART 2. DATA ANALYSIS TOOLS
# --------------------------------------------------------------------------------------------

# Now that we have a research question, selected the data set and managed our variables
# of interest and visualized their relationship graphically, we are ready to
# test those relationships statistically.

# A partir d'ici, toutes les variables doivent etre numériques.


# ---------------------------- To calulate the ANOVA F-Statistics ----------------------------------

# If we have a bi-variate statistical analysis tools for two variables i.e. y = ax + b + et
# Analysis of variace Quantiative response variable (y) and Explanatory Categorical variable (x)
# Using ols function for the computing of the F-statistic and associated p value.

# As a reminder :

print("W1_D1 is the variable is How would you rate the president at the time (Barack Obama)")
print("W1_P20 is the Personnal Annual income")
print("The income level is divided into 4 groups : 1-7 (5k- 24k), 8-11(25k-49k), 12-15(50k-99k), 16-19 (100k-175k or more))")

model1 = smf.ols(formula='W1_D1 ~ C(W1_P20)', data=df)
results1 = model1.fit()
print (results1.summary())


#%%

# To interpret this finding fully, we need to examine the actual means for the variables
# We will create a new data frame with the quatitative response variable and the categorical
# explanatory variable
sub3 = df[['W1_D1', 'W1_P20']].dropna()


# Means and standard deviations in the new dataframe :
print ("Means for income level by the president's rating")
m1= sub3.groupby('W1_P20').mean()
print (m1)


print ("standard deviations for income level by the president's rating")
sd1 = sub3.groupby('W1_P20').std()
print (sd1)

#%%
# POST HOC TEST :

# In order to conduct post hoc paired comparisons in the context of my ANOVA, examining the association
# between ethnicity and number of cigarettes smoked per month, I'm going to use
# the Tukey HSDT, or Honestly Significant Difference Test.


# mc1 is the object that will store the mutiple comparisons test
# then, I include the quantitative response variable and the categorical explanatory variable
mc1 = multi.MultiComparison(sub3['W1_D1'], sub3['W1_P20'])
res1 = mc1.tukeyhsd()       # result (mc1) x the tukey post hoc test
print(res1.summary())

#%%
# ---------------------------- Chi square test of independance ------------------------------------
# Two categorical variable

# Is the perception of the us economic situation dependent or indedendent of 
# the income levels?

# In reference to our research question, the explanatory variable is the income 
# level (W1_P20) the perception of the and the response variable nation’s 
# economic situation (measured by W1_G2 and/or W1_F1). 

# Explanatory variable, idependant (x) variable
# W1_P20: is the Personnal Annual income

# The response variables (Y):
# W1_G2: is the US economy's situation
# W1_F1: is the Percentage of how the respondants think about the future

# 1) request the contengency table of observerved counts
print("Contengency table for the US economy's situation and Personnal Annual income")
print("The first results include the table of counts of the response variable by the explanatory variable :")
count1 = pd.crosstab(df["W1_G2"], df["W1_P20"])
print(count1)

# 2) Now we have to generate the column % wich will show the % of in
print("Column percentage :")
colsum = count1.sum(axis=0)
colpercent = count1/colsum
print(colpercent)

# 3) Chi-square :
print("Chi-Square value, p-value, expected counts:")
chi_sq = scipy.stats.chi2_contingency(count1)
print(chi_sq)

# If the output is set with the explanatory variable categories across the top of the table, 
# and response variable categories down the side, it will be the column percent that we want 
# to interpret. 

#%%

# If I what to graph the percent of personnal annual income that have a positive
# view of the us economic outcoume

# C -> C plot => To plot this we need to :
# 1) set our explanatory variable (X) to categorical 
# 2) set our response variable (Y) to numeric.  
df["W1_P20"] = df["W1_P20"].astype("category")
df["W1_G2"] = pd.to_numeric(df["W1_G2"], errors="coerce")

# Plot
# X the categorical variable and Y the numeric variable
sns.factorplot(x = "W1_P20", y = "W1_G2", data=df, kind = "bar", ci=None)
plt.xlabel("The Personnal Annual income")
plt.ylabel("The US economy's situation")

# If my explanatory variably had only two levels, I could interpret the two corresponding column 
# percentages and be able to say which group had a significantly higher rate of nicotine dependents. 
# But my explanatory variable has six categories. So I know that not all are equal. But I don't 
# know which are different and which are not. 

#%%
#                               POST HOC TEST

# When the explanatory variable has more than two levels, the chi-square statistic and associated 
# p-value, do not provide insight into why the null hypothesis can be rejected. It does not tell 
# us what way the rates of nicotine dependence are not equal across the frequency categories. 

# If we reject a null hypothesis, we need to perform comparisons for each pair of nicotine 
# dependent's rates across the six smoking frequency categories.

# We need to do chi-square for each paired comparison
# We need to recode to create new variables that allow us to choose only two frequency groups at a time.

# --------------------------- Grouping values within individual variables --------------------------

# It's easyer to understant if you graph the to variables
# You need to recode for the comparaison of each "bar" of the graph
# expluding form the statement all other values in the explanatory variable (i.e. x = "W1_P20")
# We remaime this new added variable
# And run the :
# 1) request the contengency table of observerved counts
# 2) generate the column % wich will show the % of in
# 3) Compute the Chi-square :

#%%

# A reminder these are the levels of the explanatory variable : [0, 7, 11, 15, 19]
# create a new variable with only the two levels we what to test

# -------- Post Hoc test # 1 ---------
# The hard way is to do all the chi-square of each pairs and levels of the categorical variable.
# A reminder these are the levels of the explanatory variable : [0, 7, 11, 15, 19]
# create a new variable with only the two levels we what to test
# Repeat the process below :
# --------------------------- Post Hoc test # 1 to n iterations for each pair groups ---------

# 1 Recoding
# bins = {0:7, 7:11}
# df["COMP1v1"] = df["W1_P20"].map(bins)

# 2 Cotengency table of obverved counts
# count_v1 = pd.crosstab(df["W1_G2"], df["COMP1v1"])
# print(count_v1)

# 3 column percentages
# colsum_count_v1 = count_v1.sum(axis=0)
# colpct_count_v1 = count_v1/colsum
# print(colpct_count_v1)

# 4 Chi-square :
# print("Chi-Square value, p-value, expected counts:")
# chi_sq2 = scipy.stats.chi2_contingency(count_v1 )
# print(chi_sq2)

# ----- simplest way :
# ----- One can do all the pair comparison on all; but an easier way to do this is by doing this :
import itertools
for pair in itertools.combinations([0, 7, 11, 15, 19], 2):       # Calculates all the combinasions
    count_subcategory =pd.crosstab(df["W1_P20"].isin(pair), df["W1_G2"])  # and provides the cross tabs
    print("chi sq test of subcategory: {}".format(pair))
    print(scipy.stats.chi2_contingency(count_subcategory))


#%%

# One can do all the pair comparison on all; but an easier way to do this is by doing this :
import itertools
for pair in itertools.combinations([0, 7, 11, 15, 19], 2):       # Calculates all the combinasions
    count_subcategory =pd.crosstab(df["W1_P20"].isin(pair), df["W1_G2"])  # and provides the cross tabs
    print("chi sq test of subcategory: {}".format(pair))
    print(scipy.stats.chi2_contingency(count_subcategory))
#%%
# ---------------------------- Pearson Correlation -----------------------------------------------
# ---------------------------- Relationship between two quantitative variables -------------------

# Now let's find the correlation coefficients. 
# To do this, first, I create a new data frame that drops all missing, that is, N/A values for 
# each of the variables from the data set.

# The pearson coefficient can not be calculated in the presence of N/A's in the dataset :
df_clean = df.dropna()

# The scatter plot on the other hand will not be useful. In general the scatterplot is not useful
# for discrete variables (i.e. those that take on a limited number of values).

print("W1_N1J: How would you rate the wealthiest 1%?")
print("W1_N1A: How would you rate people on welfare?")
print("W1_P20 : Income level")

df_clean["W1_P20"] = pd.to_numeric(df_clean["W1_P20"], errors="coerce")

print("The association betwenn the income level and the rating of people on welfate")
print("The correlation coefficient and the associate P-value.")
print(scipy.stats.pearsonr(df_clean["W1_P20"], df_clean["W1_N1A"]))

print("The association berween the income level and the rating of the wealthiest 1%?")
print("The correlation coefficient and the assoicated P-value :")
print(scipy.stats.pearsonr(df_clean["W1_P20"], df_clean["W1_N1J"]))

print("The association between the rating of people on welfare by the wealthiest 1%")    
print("the correlation coefficient and the associated p-value. ")
print(scipy.stats.pearsonr(df_clean["W1_N1A"], df_clean["W1_N1J"]))

#%%
# ---------------------------- Exploring Statistical Interactions -------------------------------------------------

# ---------------------------- Chi square test of independance with moderation ------------------------------------
# Two categorical variable and a moderating variable

# First we need to test the existance of the relationship :

# Is the perception of the us economic situation dependent or indedendent of 
# the income levels?

# In reference to our research question, the explanatory variable is the income 
# level (W1_P20) the perception of the and the response variable nation’s 
# economic situation (measured by W1_G2 and/or W1_F1). 

# Explanatory variable, idependant (x) variable
# W1_P20: is the Personnal Annual income

# The response variables (Y):
# W1_G2: is the US economy's situation
# W1_F1: is the Percentage of how the respondants think about the future

# 1) request the contengency table of observerved counts

print("Contengency table for the US economy's situation and Personnal Annual income")
print("The first results include the table of counts of the response variable by the explanatory variable :")
count1 = pd.crosstab(df["W1_G2"], df["W1_P20"])
print(count1)

# 2) Now we have to generate the column % wich will show the % of in
print("Column percentage :")
colsum = count1.sum(axis=0)
col_percent = count1/colsum
print(col_percent)

# 3) Chi-square :
print("Chi-Square value, p-value, expected counts:")
chi_sq = scipy.stats.chi2_contingency(count1)
print(chi_sq)

# If the output is set with the explanatory variable categories across the top of the table, 
# and response variable categories down the side, it will be the column percent that we want 
# to interpret. 



# FIRST WE NEED TO DO A LITTLE BIT OF DATA MANAGEMENT

# Convert to numeric
df["W1_A1"] = pd.to_numeric(df["W1_A1"], errors = "coerce")
# Let's start the Data Management ~ decision about the data, missing values
df["W1_A1"]=df["W1_A1"].replace(-1, np.nan)

# The variables and name the count column
print("1 Extremely interested, 2 Very interested, 3 Moderately interested, 4 Slightly interested")
print("5 Not interested at all, -1 Refused")
b1 = pd.crosstab(index = df["W1_A1"], columns = "counts")
print(b1)

# 4) For the chi-square interactions : we need to define new data frames with only the levels of 
#   the moderating variables = setting new data frames that includes individuals with ore without 
# the levels of interest in politics :

sub11 = df[(df["W1_A1"]== 1)].dropna()
sub12 = df[(df["W1_A1"]== 2)].dropna()
sub13 = df[(df["W1_A1"]== 3)].dropna()
sub14 = df[(df["W1_A1"]== 4)].dropna()
sub15 = df[(df["W1_A1"]== 5)].dropna()

# Chi-Square for the relationship between INCOME LEVEL and perception of the US economy's situation
# for those who are EXTREMELY interested in politics

###             1 Extremely interested :

# 1 contingency table of observed counts for those who are EXTREMELY interested in politique
print("association between smoking quantity and nicotine dependence for those who are EXTREMELY interested in politics")
count11 = pd.crosstab(sub11["W1_G2"], sub11["W1_P20"])
print(count11)

# 2) Now we have to generate the column % wich will show the % of in
print("Column percentage :")
colsum11 = count11.sum(axis=0)
col_percent11 = count11/colsum
print(col_percent11)

# 3) Chi-square (for the US economy's situation and Personnal Annual income) for those who are EXTREMELY interested in politics:
print("Chi-square for the US economy's situation and Personnal Annual income, for those who are EXTREMELY interested in politics")
print("Chi-Square value, p-value, expected counts (EXTREMELY interested in politique):")
chi_sq11 = scipy.stats.chi2_contingency(count11)
print(chi_sq11)
print("The relationship IS NOT significative")


###             2 Very interested : 
# Chi-Square for the relationship between INCOME LEVEL and perception of the US economy's situation
# for those who are VERY INTERESTED in politics

print("Chi-square for the US economy's situation and Personnal Annual income, for those who are VERY INTERESTED in politics")
count12 = pd.crosstab(sub12["W1_G2"], sub12["W1_P20"])
print(count12)

# 2) Now we have to generate the column % wich will show the % of in
print("Column percentage :")
colsum12 = count12.sum(axis=0)
col_percent12 = count12/colsum
print(col_percent12)

# 3) Chi-square (for the US economy's situation and Personnal Annual income) for those who are VERY INTERESTED in politics:
print("Chi-square for the US economy's situation and Personnal Annual income, for those who are VERY INTERESTED in politics")
print("Chi-Square value, p-value, expected counts (EXTREMELY interested in politique):")
chi_sq12 = scipy.stats.chi2_contingency(count12)
print(chi_sq12)
print("The relationship IS NOT significative")


###             3 Moderately interested

# # Chi-Square for the relationship between INCOME LEVEL and perception of the US economy's situation
# for those who are MODERATELY INTERESTED in politics
print("Chi-square for the US economy's situation and Personnal Annual income, for those who are MODERATELY INTERESTED in politics in politics")
count13 = pd.crosstab(sub13["W1_G2"], sub13["W1_P20"])
print(count13)

# 2) Now we have to generate the column % wich will show the % of in
print("Column percentage :")
colsum13 = count13.sum(axis=0)
col_percent13 = count13/colsum
print(col_percent13)

# 3) Chi-square (for the US economy's situation and Personnal Annual income) for those who are MODERATELY INTERESTED in politics:
print("Chi-square for the US economy's situation and Personnal Annual income, for those who are MODERATELY INTERESTED in politics")
print("Chi-Square value, p-value, expected counts (MODERATELY INTERESTED in politique):")
chi_sq13 = scipy.stats.chi2_contingency(count13)
print(chi_sq13)
print("The relationship IS significative")

###             4 Slightly interested
# # Chi-Square for the relationship between INCOME LEVEL and perception of the US economy's situation
# for those who are SLIGHTLY INTERESTED in politics

print("Chi-square for the US economy's situation and Personnal Annual income, for those who are SLIGHTLY INTERESTED in politics")
count14 = pd.crosstab(sub14["W1_G2"], sub14["W1_P20"])
print(count14)

# 2) Now we have to generate the column % wich will show the % of in
print("Column percentage :")
colsum14 = count14.sum(axis=0)
col_percent14 = count14/colsum
print(col_percent14)

# 3) Chi-square (for the US economy's situation and Personnal Annual income) for those who are SLIGHTLY INTERESTED in politics:
print("Chi-square for the US economy's situation and Personnal Annual income, for those who are SLIGHTLY INTERESTED in politics")
print("Chi-Square value, p-value, expected counts (SLIGHTLY INTERESTED in politique):")
chi_sq14 = scipy.stats.chi2_contingency(count14)
print(chi_sq14)
print("The relationship IS NOT significative")

# # Chi-Square for the relationship between INCOME LEVEL and perception of the US economy's situation
# for those who are NOT INTERESTED AT ALL in politics

##              5 Not interested at all
print("Chi-square for the US economy's situation and Personnal Annual income, for those who are NOT INTERESTED AT ALL in politics")
count15 = pd.crosstab(sub15["W1_G2"], sub15["W1_P20"])
print(count15)

# 2) Now we have to generate the column % wich will show the % of in
print("Column percentage :")
colsum15 = count15.sum(axis=0)
col_percent15 = count15/colsum
print(col_percent15)

# 3) Chi-square (for the US economy's situation and Personnal Annual income) for those who are NOT INTERESTED AT ALL in politics :
print("Chi-square for the US economy's situation and Personnal Annual income, for those who are NOT INTERESTED AT ALL in politics")
print("Chi-Square value, p-value, expected counts (NOT INTERESTED AT ALL in politics):")
chi_sq15 = scipy.stats.chi2_contingency(count15)
print(chi_sq15)
print("The relationship IS NOT significative")


#%%



