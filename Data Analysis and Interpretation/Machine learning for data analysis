#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Updated on Thu Mar  8 21:02:43 2018

@author: annick-eudes
"""
# ------------------------------- PRELIMINARIES  --------------------------------

# First thing set the working directory - it is done by setting it in the folder
# icon to the right;

# Next step is to import all the library we will need

# Libraries

import pandas as pd
import numpy as np
import seaborn as sns  # for plots
import matplotlib.pyplot as plt  # as plt
import statsmodels.formula.api as smf  # statsmodels
import statsmodels.stats.multicomp as multi  # statsmodels and posthoc test
import statsmodels.api as sm  # Statsmodel for the qqplots
import scipy.stats  # For the Chi-Square test of independance

# Machine learning libraries
# Libraries for decision trees

from pandas import Series, DataFrame
import os

import sklearn.metrics

from sklearn.cross_validation import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report
from sklearn.cross_validation import train_test_split
#from sklearn.linear_model import lassoLarsCV
from sklearn.linear_model import LassoCV, LassoLarsCV, LassoLarsIC

import graphviz

# Feature Importance - for the random trees
from sklearn import datasets
from sklearn.ensemble import ExtraTreesClassifier

# This is for the machine learning with sklearn
# To indicate where the dataset is located:
os.chdir("/Users/annick/OneDrive/Documents/2. Data analysis and research/1. Data_Analysis_and_Interpretation/0. Python_Working_Directory")

# %%
# IMPORTANT BUG FIXS !!!

# Because, by default, the Pandas library often displays an abbreviated list of rows and columns
# from our data frame. And I know the number of values for some variables are fairly long,
# We are going to add additional set option statements following the library import syntax
# that requests a display of the maximum number of rows and columns.

# The default in Python, limits this display to a subset of the data frame,
# and so including display, max, columns, or rows, none, removes that limit
# and allows all rows and columns to be displayed.

# Set PANDAS to show all columns in DataFrame
pd.set_option('display.max_columns', None)

# Set PANDAS to show all rows in DataFrame
pd.set_option('display.max_rows', None)

# Bug fix for display formats to avoid run time errors
pd.set_option('display.float_format', lambda x: '%f' % x)


# ------------------------------ Machine learning codes -------------------------
# ------------------------------ Classification trees ---------------------------

# Because decision tree analyses cannot handle any NA's in our data set,
# The next step is to create a clean data frame that drops all NA's.
# Importing the data set:

df = pd.read_csv("ool_pds.csv", low_memory=False)

# Let's do a bit of data management on the variables that we will be working with
df["W1_P20"] = df["W1_P20"].replace(-1, np.nan)
df["W1_G2"] = df["W1_G2"].replace(-1, np.nan)
df["W1_F1"] = df["W1_F1"].replace(-1, np.nan)
df["W1_D1"] = df["W1_D1"].replace(-1, np.nan)
df["W1_D1"] = df["W1_D1"].replace(998, np.nan)
df["W1_A12"] = df["W1_A12"].replace(-1, np.nan)
df["W1_C1"] = df["W1_C1"].replace(-1, np.nan)
df["W1_A1"] = df["W1_A1"].replace(-1, np.nan)
df["W1_M1"] = df["W1_M1"].replace(-1, np.nan)
df["W1_P11"] = df["W1_P11"].replace(-1, np.nan)

# Getting the information about the dataset
df.info()

"""
Running a Classification Tree:
We will need to perform a decision tree analysis to test nonlinear relationships 
among a series of explanatory variables and a binary, categorical response variable.


"""
# Let's create a new variable from the 3 level target variable :

def economy_situation(row):
    if row["W1_G2"] == 1:
        return 1
    if row["W1_G2"] == 2:
        return 0
    if row["W1_G2"] == 3:
        return 0
# This variable is then added to the dataframe
df["economy_situation"] = df.apply(lambda row: economy_situation(row), axis=1)
#%%
df["economy_situation"] = df["economy_situation"].astype("category")

# Just to verify if the variable has been added to the dataset :
df.info()

print("This is the counts for the economy_situation : 1 = Better, 0 = Worse")
count_economy_situation = pd.crosstab(index=df["economy_situation"], columns="counts")
print(count_economy_situation)

df_clean.dtypes
df_clean.describe()

"""
Next, we need to set our explanatory (X) 
and response or target variables (Y); and then include the train test split function for predictors and target.
"""
predictors = df_clean[["W1_A1" ,"W1_C1"]]
targets = df_clean.economy_situation  # ==> accuracy_score = 0.77 in a other attempt?!

# 2) Split into training and testing sets


# And then include the train test split function for predictors and target.
# And set the size ratio to 60 % for the training sample and 40% for the test sample 
# by indicating test_size=.4. 

# I'll do a 50/50 split of the sample

pred_train, pred_test, tar_train, tar_test  =   train_test_split(predictors, targets, test_size= 0.4)
# 3) Here I request the shape of these predictor and target training and
# test samples.

# This is the training sample : observations (% of the ratio of our original sample, and nb of explanatory variables")
print("Training sample : observations and explanatory variables")
print(pred_train.shape)

print("The test sample : observations and explanatory variables")
print(pred_test.shape)

# Note that the total should give you the sample size, since whe decided to plit the sample into
# 0.4 for the test sample and 0.6 for the training sample
# Test it by doing len(df) lenght of the dataset

#%%
tar_train.shape
#%%
tar_test.shape
#%%

# Build the decision tree classifier model on training data
classifier = DecisionTreeClassifier()
#%%
# We Then use this classifier.fit function which we pass the training predictors and
# training targets to
classifier = classifier.fit(pred_train,tar_train)
#%%
# Next we include the predict function where we predict for the test values
predictions = classifier.predict(pred_test)
#%%

# then call in the confusion matrix function which we passed the target test sample to.
# This shows the correct and incorrect classifications of our decision tree.
# To check if the result make sence, at least, we should have the same number of labels of the 
# target variable, hase we have of columns

sklearn.metrics.confusion_matrix(tar_test,predictions)

#%%
# We can also look at the accuracy score which is approximately 0.77,
# which suggests that the decision tree model has classified 77%
# of the sample correctly as either regular or not regular smokers.

print("This is the accuracy score of the classification model :")
sklearn.metrics.accuracy_score(tar_test, predictions)

# Classification
#D isplaying the decision tree
from sklearn import tree
#from StringIO import StringIO

from io import StringIO
#from StringIO import StringIO 

from IPython.display import Image
out = StringIO()

# This is the code to export the decision tree image in the folder where the notebook is lockated : out_file = None
# image_tree = tree.export_graphviz(classifier, out_file = None)
# graph = graphviz.Source(image_tree)
# graph.render("decision tree for the assignment")

# This is to code to have the decision tree image in the notbook :
tree.export_graphviz(classifier, out_file=out)

import pydotplus
graph=pydotplus.graph_from_dot_data(out.getvalue())
Image(graph.create_png())


#%%
"""
# ----------------------------  Building a Random Forest ----------------------
# The research question is :
# To what extent is the perception of the US situation (W1_G2) associated with the level of income (W1_P20)?

# The variables of interest in our research question

print("W1_P20 is the Personnal Annual income")
print("W1_G2 is the US economy's situation")
print("W1_F1 is the Percentage of how the respondants think about the future")
print("")
# Determining the number of rows and columns in the dataset


Explanatory variables :
# Target : economy's situation 
    

# W1_G2 Now thinking about the country's economy, would you say that compared to one year ago, the nation's economy is now better, about the same, or worse?

# Predictors (Xi) that we have tested
# W1_F1 When you think about your future, are you generally optimistic, pessimistic, or neither optimistic nor pessimistic?
# Don't add W1_F1 as a predictor
# PPETHM: Race / Ethnicity
# W1_P20 Which of the following income groups includes YOUR personal annual income (Do not include the income of other members of your household)?
# PPINCIMP: Household Income
# W1_C1: Generally speaking, do you usually think of yourself as a Democrat, a Republican, an Independent, or something else?
# W1_A1 How interested are you in whatâ€™s going on in government and politics?
# W1_D1 : [Barack Obama] How would you rate:
# W1_M1: What is your religion?
# W1_P11: Is anyone in your household currently unemployed?
# PPAGE: Age
# PPAGECAT: Age - 7 Categories



"""
#%%
"""
Running a random forest;
among a series of explanatory variables and a binary, categorical response variable.

"""

def economy_situation(row):
    if row["W1_G2"] == 1:
        return 1
    if row["W1_G2"] == 2:
        return 0
    if row["W1_G2"] == 3:
        return 0
# This variable is then added to the dataframe
df["economy_situation"] = df.apply(lambda row: economy_situation(row), axis=1)
#%%
df["economy_situation"] = df["economy_situation"].astype("category")

#%%
# Because random forest cannot handle any NA's in the data set,
# the next step is to create a clean data frame that drops all NA's.
 
df_clean = df.dropna()      # Because decision trees cannot handel na's

#%%
# To have a look at the data types and the summary statistics
df_clean.dtypes
df_clean.describe()

# Next, we need to set our explanatory and response or target variables (Y).
# And then include the train test split function for predictors and target.
#%%
# The we set our predictor and target variables => our explanatory and response
# variables

# predictors = df_clean[["W1_A1" ,"W1_C1"]] this gives an accuracy score of 0.749
#predictors = df_clean[["W1_A1" ,"W1_C1", "W1_P20"]] this gives an accuracy score of 0.69250

predictors = df_clean[["W1_A1" ,"W1_C1", "W1_P20", "PPAGE", "PPINCIMP", "W1_P11", "PPETHM"]]
targets = df_clean.economy_situation  # ==> accuracy_score = 0.749 or 0.77 in a other attempt?!
#%%
"""
Next I set my explanatory and response, or target variables, and
then include the train test split function for predictors and target.
And set the size ratio to 60% for the training sample, and 40% for
the test sample.

"""

pred_train, pred_test, tar_train, tar_test  =   train_test_split(predictors, targets, test_size= 0.4)
# 3) Here I request the shape of these predictor and target training and
# test samples.

# This is the training sample : observations (% of the ratio of our original sample, and nb of explanatory variables")
print("Training sample : observations and explanatory variables")
print(pred_train.shape)

print("The test sample : observations and explanatory variables")
print(pred_test.shape)
#%%
# From the sklearn library, we'll import the RandomForestClassifier
from sklearn.ensemble import RandomForestClassifier

#%%
"""
Now that training and test data sets have already been created, we'll initialize
 the random forest classifier from SK Learn and indicate n_estimators= nb of variables.
 n_estimators are the number of trees you would build with the random forest
 algorithm.
 """
classifier = RandomForestClassifier(n_estimators = 7)
classifier = classifier.fit(pred_train, tar_train)

# Then we do the prediction on the test data set
predictions = classifier.predict(pred_test)

#%%
# Confusion matrix and accuracy scores
print("This is the confusion matrix :")
sklearn.metrics.confusion_matrix(tar_test, predictions)

#%%
print("This is the accuracy score :")
sklearn.metrics.accuracy_score(tar_test, predictions)
#%%
"""
74% of the individuals where classified correctly.
Given that we don't interpret individual trees in a random forest,
the most helpful information to be gotten from a forest
is the measured importance for each explanatory variable.
==> orthe features.
 Based on how many votes or splits each has produced in the 2 tree ensemble.
 """
 
model = ExtraTreesClassifier()
model.fit(pred_train, tar_train)
print(model.feature_importances_)

#%%

"""
Bases on the results the most important explanatory variable is the political
afficilation in determining the perception of the US economy's situation.

So were "n" number of trees actually needed to get this correct rate of classification?
To determine what growing larger number of trees has brought us in terms of correct
classification.
We're going to use code that builds for us different numbers of trees,
from one to "n", and provides the correct classification rate for each.
This code will build for us random forest classifier from one to "n",
and then finding the accuracy score for each of those trees from one to "n",
and storing it in an array.

Running a different number of trees and see the effect
 of that on the accuracy of the prediction
"""

trees=range(4)
accuracy=np.zeros(4)

for idx in range(len(trees)):
   classifier=RandomForestClassifier(n_estimators=idx + 1)
   classifier=classifier.fit(pred_train,tar_train)
   predictions=classifier.predict(pred_test)
   accuracy[idx]=sklearn.metrics.accuracy_score(tar_test, predictions)
   
plt.cla()
plt.plot(trees, accuracy)

"""
From the plot of the accuracy score when we add new trees in the forest
We don't need all these variables to predict the perception of the US economy's
situation.
"""
# ----------------------------  Running a Lasso Regression Analysis ---------------------------------------------------------------

# Loading the dataset

df = pd.read_csv("ool_pds.csv", low_memory=False)
df.head(5)

#%%

df["W1_P20"] = df["W1_P20"].replace(-1, np.nan)
df["W1_G2"] = df["W1_G2"].replace(-1, np.nan)
df["W1_F1"] = df["W1_F1"].replace(-1, np.nan)
df["W1_D1"] = df["W1_D1"].replace(-1, np.nan)
df["W1_D1"] = df["W1_D1"].replace(998, np.nan)
df["W1_A12"] = df["W1_A12"].replace(-1, np.nan)
df["W1_C1"] = df["W1_C1"].replace(-1, np.nan)
df["W1_A1"] = df["W1_A1"].replace(-1, np.nan)
df["W1_M1"] = df["W1_M1"].replace(-1, np.nan)
df["W1_P11"] = df["W1_P11"].replace(-1, np.nan)
df["PPINCIMP"] = df["PPINCIMP"].replace(-1 , np.nan)
df["PPINCIMP"] = df["PPINCIMP"].replace(-2, np.nan)
df["PPETHM"] = df["PPETHM"].replace(-1, np.nan)
df["PPETHM"] = df["PPETHM"].replace(-2, np.nan)

#%%

# The response variable in our research question

def economy_situation(row):
    if row["W1_G2"] == 1:
        return 1
    if row["W1_G2"] == 2:
        return 0
    if row["W1_G2"] == 3:
        return 0

# This variable is then added to the dataframe
df["economy_situation"] = df.apply(lambda row: economy_situation(row), axis=1)

df["economy_situation"] = df["economy_situation"].astype("category")

df_clean = df.dropna()

#%%

# Select predictor variables and target variable as separate data sets  
predvar = df_clean[["W1_A1", "W1_C1", "W1_P20", "PPAGE", "PPINCIMP", "W1_P11", "PPETHM"]]

# This is the target variable
target = df_clean.economy_situation

#%%

# standardize predictors to have mean=0 and sd=1
predictors = predvar.copy()

from sklearn import preprocessing
#%%
predictors["W1_A1"] = preprocessing.scale(predictors["W1_A1"].astype("float64"))
predictors["W1_C1"] = preprocessing.scale(predictors["W1_C1"].astype("float64"))
predictors["W1_P20"] = preprocessing.scale(predictors["W1_P20"].astype("float64"))
predictors["PPAGE"] = preprocessing.scale(predictors["PPAGE"].astype("float64"))
predictors["PPINCIMP"] = preprocessing.scale(predictors["PPINCIMP"].astype("float64"))
predictors["W1_P11"] = preprocessing.scale(predictors["W1_P11"].astype("float64"))
predictors["PPETHM"] = preprocessing.scale(predictors["PPETHM"].astype("float64"))

#%%
# split data into train and test sets
pred_train, pred_test, tar_train, tar_test = train_test_split(predictors, 
                                                              target, 
                                                              test_size=.3, 
                                                              random_state=123)


#%%
# specify the lasso regression model
model=LassoLarsCV(cv=10, precompute=False).fit(pred_train,tar_train)

# print variable names and regression coefficients
dict(zip(predictors.columns, model.coef_))

# plot coefficient progression
m_log_alphas = -np.log10(model.alphas_)
ax = plt.gca()
plt.plot(m_log_alphas, model.coef_path_.T)
plt.axvline(-np.log10(model.alpha_), linestyle='--', color='k',
            label='alpha CV')
plt.ylabel('Regression Coefficients')
plt.xlabel('-log(alpha)')
plt.title('Regression Coefficients Progression for Lasso Paths')

#%%
# plot mean square error for each fold
m_log_alphascv = -np.log10(model.cv_alphas_)
plt.figure()
plt.plot(m_log_alphascv, model.cv_mse_path_, ':')
plt.plot(m_log_alphascv, model.cv_mse_path_.mean(axis=-1), 'k',
         label='Average across the folds', linewidth=2)
plt.axvline(-np.log10(model.alpha_), linestyle='--', color='k',
            label='alpha CV')
plt.legend()
plt.xlabel('-log(alpha)')
plt.ylabel('Mean squared error')
plt.title('Mean squared error on each fold')
         
#%%
# MSE from training and test data
from sklearn.metrics import mean_squared_error
train_error = mean_squared_error(tar_train, model.predict(pred_train))
test_error = mean_squared_error(tar_test, model.predict(pred_test))
print ('training data MSE')
print(train_error)
print ('test data MSE')
print(test_error)
#%%
# R-square from training and test data
rsquared_train=model.score(pred_train,tar_train)
rsquared_test=model.score(pred_test,tar_test)
print ('training data R-square')
print(rsquared_train)
print ('test data R-square')
print(rsquared_test)

#%%
